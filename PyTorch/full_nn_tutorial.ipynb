{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "imposed-india",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} -c anaconda requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-bottom",
   "metadata": {},
   "source": [
    "## In this notebook, we are creating neural networks and applying them to MNIST\n",
    "#### It consists on black and white images of hand-drawn digits (between 0 and 9)\n",
    "### Updated from https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "adjacent-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-opportunity",
   "metadata": {},
   "source": [
    "### The dataset is in pickle format, so we load it\n",
    "#### The arrays are in numpy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "premier-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-continent",
   "metadata": {},
   "source": [
    "#### Let's display a number from the dataset shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "boring-black",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-chemical",
   "metadata": {},
   "source": [
    "### We convert the numpy array to a Pytorch tensor,\n",
    "### By using the torch.tensor command inside the \"map\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "brazilian-updating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-customer",
   "metadata": {},
   "source": [
    "# We build a Neural Net froms scratch <br /> \n",
    "We use the Xavier Initialization <br /> \n",
    "which consists in multiplying with 1/sqrt(n)) <br /> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "enormous-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-petroleum",
   "metadata": {},
   "source": [
    "#This cell shows how to build and activation function with\n",
    "#Pytorch\n",
    "\n",
    "The second function is the model and the \"@\" stands for the dot product  <br /> \n",
    "That leads to an activation function.  <br /> \n",
    "Note that we start with random weights  <br /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "collective-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "relevant-element",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.0533, -2.0762, -1.7326, -2.3711, -2.2983, -2.3505, -2.4969, -2.7050,\n",
      "        -1.9446, -2.6712], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-xerox",
   "metadata": {},
   "source": [
    "### The tensor also contains a gradient function\n",
    "### that will be later used for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "reverse-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "warming-clearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.0533, -2.0762, -1.7326, -2.3711, -2.2983, -2.3505, -2.4969, -2.7050,\n",
      "        -1.9446, -2.6712], grad_fn=<SelectBackward>) torch.Size([64, 10])\n",
      "tensor(2.4014, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)\n",
    "\n",
    "#We check our loss with our random model\n",
    "\n",
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "opening-dream",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0625)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "print(accuracy(preds, yb))\n",
    "\n",
    "#We check the accuracy of the random model,\n",
    "#which is pretty low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-symphony",
   "metadata": {},
   "source": [
    "### We run a training loop and for each iteration we will\n",
    "\n",
    "(1) select a mini-batch of data (of size bs) <br /> \n",
    "(2) use the model to make predictions <br /> \n",
    "(3) calculate the loss <br /> \n",
    "(4) loss.backward() updates the gradients of the model, in this case, weights and bias <br /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "secondary-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 10  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-pound",
   "metadata": {},
   "source": [
    "### We now have a minimal neural net!!  \n",
    "### Let's print out the loss and the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "creative-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0549, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-naples",
   "metadata": {},
   "source": [
    "### Let's now take advantage of the torch neural net libraries via torch functional\n",
    "### torch.nn.functional\n",
    "\n",
    "It has a wide a range of loss and activation fuctions  <br />  \n",
    "As well as ways to create its neural net\n",
    "\n",
    "(1) select a mini-batch of data (of size bs) <br /> \n",
    "(2) F.cross_entropy allows for a convenient wayt to combine  <br /> \n",
    "    negative log likelihood loss and log softmax activation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "alert-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "heavy-tobago",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0549, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-approval",
   "metadata": {},
   "source": [
    "### Let's now explore the nn.Module\n",
    "It allows to keep track of the weights, bias and methods  <br /> \n",
    "for the forward step  <br /> \n",
    "So we can update our parameters in a more concise way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ideal-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "plain-stroke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3585, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Let's instantiate our model\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "with torch.no_grad():\n",
    "    weights -= weights.grad * lr\n",
    "    bias -= bias.grad * lr\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "moving-visibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0556, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-miniature",
   "metadata": {},
   "source": [
    "With this model, our loss has gone significantly  <br />\n",
    "down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-latin",
   "metadata": {},
   "source": [
    "## Let's now explore nn.linear\n",
    "Applies a linear transformation to the incoming data forllowing this model: y = xA^T + b <br /> \n",
    "Precesily \"xb  @ self.weights + self.bias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "unlimited-washington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3718, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "\n",
    "#Instantiation of the model and calculation of the loss\n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "opened-contrary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our loss using the nn.linear model is  tensor(0.0554, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "\n",
    "print(\"Our loss using the nn.linear model is \", loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-germany",
   "metadata": {},
   "source": [
    "### Let's take advantage of the optimization\n",
    "We will use the \"torch.optim\" package  <br /> \n",
    "Thanks to that package we will go from : <br /> \n",
    "\n",
    "with torch.no_grad(): <br /> \n",
    "    for p in model.parameters(): p -= p.grad * lr <br /> \n",
    "    model.zero_grad() <br /> \n",
    "    \n",
    "to just 2 lines of code!!: <br /> \n",
    "opt.step()  <br /> \n",
    "opt.zero_grad()  <br /> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "average-extraction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3438, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0553, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-cycle",
   "metadata": {},
   "source": [
    "## Let's use the Dataset class\n",
    "(1) It comes with built-in method __len__(called by Python’s standard len function)  <br />\n",
    "(2) and a __getitem__ function as a way of indexing into it.  <br />\n",
    "(3) Both xtrain and ytrain will be combined as a single Tensor Dataset  <br />\n",
    "    which makes it easier to iterate and slice  <br />\n",
    "    \n",
    "    \n",
    "##############  <br />\n",
    "\n",
    "Essentially:  <br />\n",
    "xb = x_train[start_i:end_i]  <br />\n",
    "yb = y_train[start_i:end_i]  <br />\n",
    "\n",
    "becomes  <br />\n",
    "\n",
    "xb,yb = train_ds[i*bs : i*bs+bs]  <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "copyrighted-difference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0554, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "\n",
    "xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-pasta",
   "metadata": {},
   "source": [
    "## We can also get the minibatch automatically\n",
    "(1) It comes with built-in method __len__(called by Python’s standard len function)  <br />\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "recent-modification",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0556, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "\n",
    "for xb,yb in train_dl:\n",
    "    pred = model(xb)\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-reserve",
   "metadata": {},
   "source": [
    "## Let's add the validation set to prevent the correlation between\n",
    "batches and overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "broadband-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-louis",
   "metadata": {},
   "source": [
    "## We print validation loss after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "perfect-healing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2923)\n",
      "1 tensor(0.2948)\n",
      "2 tensor(0.2718)\n",
      "3 tensor(0.2757)\n",
      "4 tensor(0.2784)\n",
      "5 tensor(0.2678)\n",
      "6 tensor(0.2794)\n",
      "7 tensor(0.3119)\n",
      "8 tensor(0.2683)\n",
      "9 tensor(0.2810)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-shuttle",
   "metadata": {},
   "source": [
    "### We calculating the loss for both the training set and the validation set,\n",
    "## and create a loss function for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "minor-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n",
    "        \n",
    "        \n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-management",
   "metadata": {},
   "source": [
    "### We may now fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "baking-cabinet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3160462842941284\n",
      "1 0.2942699025392532\n",
      "2 0.2914047648489475\n",
      "3 0.27732405828237533\n",
      "4 0.28337413132190703\n",
      "5 0.27754870800971987\n",
      "6 0.2759702271938324\n",
      "7 0.27281663073301315\n",
      "8 0.30301346428394316\n",
      "9 0.2936370015859604\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-setup",
   "metadata": {},
   "source": [
    "## We may now switch to CNN in 2D\n",
    "We will create 3 convolutional layers  <br />\n",
    "We use ReLU as an activation fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "super-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-worcester",
   "metadata": {},
   "source": [
    "## We take into account the momentum\n",
    "which is a variation on stochastic gradient descent that  <br />\n",
    "takes previous updates into account as well and generally leads to faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "suspected-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3326562440872192\n",
      "1 0.2613667967557907\n",
      "2 0.23306234071552753\n",
      "3 0.18285769625902176\n",
      "4 0.18630328620672226\n",
      "5 0.1534605371236801\n",
      "6 0.14499054169356823\n",
      "7 0.14741806494891643\n",
      "8 0.17534390926361085\n",
      "9 0.13108132175803183\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-foster",
   "metadata": {},
   "source": [
    "###  Let's use nn.Sequential\n",
    "Sequential object runs each of the modules contained within it, <br />\n",
    "in a sequential manner. It makes it simpler to write our neural network (NN)<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "interior-night",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3805970980644226\n",
      "1 0.2464430647134781\n",
      "2 0.23881264120340348\n",
      "3 0.24158590399324895\n",
      "4 0.16752589990198613\n",
      "5 0.1699852031648159\n",
      "6 0.1422518071293831\n",
      "7 0.13342807356119155\n",
      "8 0.14889539048075676\n",
      "9 0.14919606731235982\n"
     ]
    }
   ],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-quarterly",
   "metadata": {},
   "source": [
    "###  One thing with our CNN function to keep in mind is that:\n",
    "(1) It assumes the input is a 28*28 long vector <br />\n",
    "(2) It assumes that the final CNN grid size is 4*4 (since that’s the average <br />\n",
    "    Sequential object runs each of the modules contained within it, <br />\n",
    "#Let;'s define our module in a different way'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "generic-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-coach",
   "metadata": {},
   "source": [
    "We essentially can replace \"nn.AvgPool2d\" with \"nn.AdaptiveAvgPool2d\" <br />\n",
    "in order to  define the size of the output tensor we want <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "prepared-coating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5901534338951111\n",
      "1 0.49427659039497374\n",
      "2 0.4860578450202942\n",
      "3 0.4295502504825592\n",
      "4 0.4432272716999054\n",
      "5 0.4297263397693634\n",
      "6 0.4117113856315613\n",
      "7 0.40519885873794553\n",
      "8 0.39270885734558103\n",
      "9 0.39931350069046023\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch_gpu_par]",
   "language": "python",
   "name": "conda-env-.conda-torch_gpu_par-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
